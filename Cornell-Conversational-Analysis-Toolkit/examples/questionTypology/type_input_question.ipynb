{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishaanjhaveri/anaconda3/lib/python3.6/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pkg_resources\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from convokit import Corpus, QuestionTypology, download, MotifsExtractor, QuestionTypologyUtils\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from ast import literal_eval as make_tuple\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from spacy.en import English\n",
    "from spacy.symbols import *\n",
    "from spacy.tokens.doc import Doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize QuestionTypology class pretrained on Parliament Dataset\n",
    "\n",
    "num_clusters = 8\n",
    "\n",
    "data_dir = os.path.join(pkg_resources.resource_filename(\"convokit\", \"\"), 'downloads', 'parliament')\n",
    "motifs_dir = os.path.join(data_dir, 'parliament-motifs')\n",
    "\n",
    "corpus = Corpus(filename=os.path.join(data_dir, 'parliament-corpus'))\n",
    "\n",
    "questionTypology = QuestionTypology(corpus, data_dir, dataset_name='parliament', motifs_dir=motifs_dir, num_dims=25,\n",
    "  num_clusters=num_clusters, verbose=False, random_seed=164)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "#create spacy object\n",
    "spacy_NLP = spacy.load('en')\n",
    "vocab = English().vocab\n",
    "\n",
    "question_fit_file = os.path.join(questionTypology.motifs_dir, 'question_fits.json')\n",
    "\n",
    "superset_file = os.path.join(questionTypology.motifs_dir, 'question_supersets_arcset_to_super.json')\n",
    "\n",
    "question_to_leaf_fits = []\n",
    "\n",
    "question_threshold = questionTypology.question_threshold\n",
    "\n",
    "super_mappings = {}\n",
    "with open(superset_file) as f:\n",
    "    for line in f.readlines():\n",
    "        entry = json.loads(line)\n",
    "        super_mappings[tuple(entry['arcset'])] = tuple(entry['super'])\n",
    "\n",
    "with open(question_fit_file) as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        entry = json.loads(line)\n",
    "        motif = tuple(entry['arcset'])\n",
    "        super_motif = super_mappings[motif]\n",
    "        if entry['arcset_count'] < question_threshold: continue\n",
    "        if entry['max_valid_child_count'] < question_threshold:\n",
    "            question_to_leaf_fits.append(super_motif)\n",
    "\n",
    "# if none of its children are in all_motifs, increment question_matrix\n",
    "# else recurse on those children that are in all_motifs\n",
    "def identify_sinks(parent, relevant_children, downlinks, question_matrix, all_motifs):\n",
    "    children_in_all_motifs = [motif in all_motifs and motif != parent for motif in relevant_children]\n",
    "    if any(children_in_all_motifs):\n",
    "        for i in range(len(relevant_children)):\n",
    "            if children_in_all_motifs[i]:\n",
    "                identify_sinks(relevant_children[i], list(downlinks[relevant_children[i]].keys()), downlinks, question_matrix, all_motifs)\n",
    "    else:\n",
    "        j = all_motifs.index(parent)\n",
    "        question_matrix[j] = 1\n",
    "\n",
    "def compute_question_matrix(question_text):\n",
    "        '''\n",
    "            Helper function to classify_question. Computes and returns a representation of\n",
    "            question_text as a matrix in the latent space\n",
    "        '''\n",
    "        spacy_q_obj = Doc(vocab).from_bytes(spacy_NLP(question_text).to_bytes())\n",
    "\n",
    "        #extract question fragments\n",
    "        for span_idx, span in enumerate(spacy_q_obj.sents):\n",
    "            curr_arcset = MotifsExtractor.get_arcs(span.root, True)\n",
    "            fragments = list(curr_arcset)\n",
    "        fragment_dict = {}\n",
    "        fragment_dict['1'] = list(fragments)\n",
    "        itemset_counts, span_to_itemsets = MotifsExtractor.count_frequent_itemsets(fragment_dict, \n",
    "                                                                                   questionTypology.min_support, \n",
    "                                                                                   questionTypology.item_set_size, \n",
    "                                                                                   questionTypology.verbose)\n",
    "\n",
    "        \n",
    "        itemsets = []\n",
    "        for count in itemset_counts:\n",
    "            for itemset in itemset_counts[count]:\n",
    "                if itemset in question_to_leaf_fits:\n",
    "                    itemsets.append(itemset)\n",
    "        \n",
    "        new_itemset_counts = {}\n",
    "        for setsize, size_dict in itemset_counts.items():\n",
    "            for k,v in size_dict.items():\n",
    "                new_itemset_counts[k] = v\n",
    "        itemset_counts = new_itemset_counts\n",
    "        itemset_counts[('*',)] = len(fragment_dict)\n",
    "\n",
    "        sorted_counts = sorted(itemset_counts.items(),key=lambda x: (-x[1],len(x[0]),x[0][0]))\n",
    "\n",
    "        edges = []\n",
    "        uplinks = defaultdict(dict)\n",
    "        downlinks = defaultdict(dict)\n",
    "\n",
    "        for itemset,count in itemset_counts.items():\n",
    "            parents = []\n",
    "            set_size = len(itemset)\n",
    "            if set_size == 1:\n",
    "                arc = itemset[0]\n",
    "                if arc.endswith('*'):\n",
    "                    parents.append(('*',))\n",
    "                elif '_' in arc:\n",
    "                    parents.append((arc.split('_')[0] + '_*',))\n",
    "                elif '>' in arc:\n",
    "                    parents.append((arc.split('>')[0] + '>*',))\n",
    "\n",
    "            else:\n",
    "                for idx in range(set_size):\n",
    "                    parents.append(itemset[:idx] + itemset[idx+1:])\n",
    "            for parent in parents:\n",
    "                parent_count = itemset_counts[parent]\n",
    "                pr_child = count / itemset_counts[parent]\n",
    "                edges.append({'child': itemset, 'child_count': count,\n",
    "                            'parent': parent, 'parent_count': parent_count,\n",
    "                            'pr_child': pr_child})\n",
    "                uplinks[itemset][parent] = {'pr_child': pr_child, 'parent_count': parent_count}\n",
    "                downlinks[parent][itemset] = {'pr_child': pr_child, 'child_count': count}\n",
    "\n",
    "\n",
    "        all_motifs = list(questionTypology.mtx_obj['q_terms'])\n",
    "    \n",
    "        # create question_matrix\n",
    "        question_matrix = np.zeros((questionTypology.num_motifs, 1))\n",
    "        identify_sinks(('*',), list(downlinks[('*',)].keys()), downlinks, question_matrix, all_motifs)\n",
    "        question_matrix = Normalizer(norm=questionTypology.norm).fit_transform(question_matrix)\n",
    "        return question_matrix\n",
    "\n",
    "def classify_question(question_text):\n",
    "        '''\n",
    "            Returns the type of question_text\n",
    "        '''\n",
    "        question_matrix = compute_question_matrix(question_text)\n",
    "        mtx = np.matmul(question_matrix.T, questionTypology.lq)\n",
    "        label = questionTypology.km.predict(mtx)\n",
    "        return question_matrix, mtx, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine type of input question\n",
    "\n",
    "example_question = \"Does my right hon Friend agree that excellent regional universities—for example , the University of Northumbria at Newcastle and Sunderland—are anxious that they will be at a disadvantage if an élite group of universities , mainly in the south - east of England , are allowed to raise their fees to figures upwards of £ 10,000 a year , as today 's newspapers reported the Minister for Lifelong Learning and Higher Education as saying ?\"\n",
    "# example_question = \"What is the minister going to do about?\"\n",
    "question_matrix, mtx, label = classify_question(example_question)\n",
    "print('Question: ', example_question)\n",
    "print('Cluster: ', label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
